# features.py
import re
import socket
from urllib.parse import urlparse
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import whois
import dns.resolver
from functools import lru_cache


# 30 ä¸ªç‰¹å¾åç§°ï¼ˆæœ€åä¸€ä¸ªæ˜¯æ ‡ç­¾ï¼Œåœ¨è¿™é‡Œä¸ç”Ÿæˆï¼‰
FEATURE_NAMES = [
    "having_IP_Address",
    "URL_Length",
    "Shortining_Service",
    "having_At_Symbol",
    "double_slash_redirecting",
    "Prefix_Suffix",
    "having_Sub_Domain",
    "SSLfinal_State",
    "Domain_registeration_length",
    "Favicon",
    "port",
    "HTTPS_token",
    "Request_URL",
    "URL_of_Anchor",
    "Links_in_tags",
    "SFH",
    "Submitting_to_email",
    "Abnormal_URL",
    "Redirect",
    "on_mouseover",
    "RightClick",
    "popUpWidnow",
    "Iframe",
    "age_of_domain",
    "DNSRecord",
    "web_traffic",
    "Page_Rank",
    "Google_Index",
    "Links_pointing_to_page",
    "Statistical_report",
]


# ---------------------------------------------------
# ä¸€äº›å·¥å…·å‡½æ•°
# ---------------------------------------------------

SHORTENING_SERVICES = [
    "bit.ly", "tinyurl.com", "goo.gl", "ow.ly", "t.co",
    "is.gd", "buff.ly", "adf.ly", "bit.do", "cutt.ly"
]

def get_domain(url):
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""

def get_scheme(url):
    try:
        return urlparse(url).scheme.lower()
    except Exception:
        return ""

def get_path(url):
    try:
        return urlparse(url).path.lower()
    except Exception:
        return ""

# ---------------------------------------------------
# 1. having_IP_Address
# ---------------------------------------------------
def feat_having_IP_Address(url):
    domain = get_domain(url)
    # åŒ¹é… IPv4 æˆ– IPv6
    ipv4_pattern = r"^\d{1,3}(\.\d{1,3}){3}$"
    ipv6_pattern = r"^\[?[0-9a-fA-F:]+\]?$"
    if re.match(ipv4_pattern, domain) or re.match(ipv6_pattern, domain):
        return 1
    return -1

# ---------------------------------------------------
# 2. URL_Length
# ---------------------------------------------------
def feat_URL_Length(url):
    length = len(url)
    if length < 54:
        return -1
    elif 54 <= length <= 75:
        return 0
    else:
        return 1

# ---------------------------------------------------
# 3. Shortining_Service
# ---------------------------------------------------
def feat_Shortining_Service(url):
    domain = get_domain(url)
    if any(svc in domain for svc in SHORTENING_SERVICES):
        return 1
    return -1

# ---------------------------------------------------
# 4. having_At_Symbol
# ---------------------------------------------------
def feat_having_At_Symbol(url):
    return 1 if "@" in url else -1

# ---------------------------------------------------
# 5. double_slash_redirecting
# ---------------------------------------------------
def feat_double_slash_redirecting(url):
    # æŸ¥æ‰¾é™¤ "http://" ä¹‹å¤–çš„ "//" ä½ç½®
    pos = url.find("//", 7)
    if pos != -1:
        return 1
    return -1

# ---------------------------------------------------
# 6. Prefix_Suffixï¼ˆåŸŸåä¸­ä½¿ç”¨ - ï¼‰
# ---------------------------------------------------
def feat_Prefix_Suffix(url):
    domain = get_domain(url)
    if "-" in domain.split(".")[0]:
        return 1
    return -1

# ---------------------------------------------------
# 7. having_Sub_Domain
# ---------------------------------------------------
def feat_having_Sub_Domain(url):
    domain = get_domain(url)
    # å»æ‰ç«¯å£
    domain = domain.split(":")[0]
    dots = domain.count(".")
    if dots <= 1:
        return -1
    elif dots == 2:
        return 0
    else:
        return 1

# ---------------------------------------------------
# 8. SSLfinal_Stateï¼ˆç®€åŒ–ç‰ˆï¼‰
# ---------------------------------------------------
def feat_SSLfinal_State(url):
    scheme = get_scheme(url)
    if scheme != "https":
        return 1
    # ç®€å•è®¤ä¸º https å°±ç»™ -1ï¼Œå¤æ‚ä¸€ç‚¹å¯ä»¥åŠ è¯ä¹¦æ£€æŸ¥
    return -1

# ---------------------------------------------------
# 9. Domain_registeration_lengthï¼ˆWHOISï¼Œç®€åŒ–ï¼‰
# ---------------------------------------------------
def feat_Domain_registeration_length(url):
    domain = get_domain(url).split(":")[0]
    try:
        w = whois.whois(domain)
        exp = w.expiration_date
        if isinstance(exp, list):
            exp = exp[0]
        created = w.creation_date
        if isinstance(created, list):
            created = created[0]
        if not exp or not created:
            return 0
        if (exp - created).days / 365.0 <= 1:
            return 1
        else:
            return -1
    except Exception:
        return 0  # æœªçŸ¥

# ---------------------------------------------------
# 10. Faviconï¼ˆç®€åŒ–ï¼šfavicon æ˜¯å¦åœ¨åŒåŸŸï¼‰
# ---------------------------------------------------
def feat_Favicon(url, soup):
    try:
        domain = get_domain(url)
        icon_link = soup.find("link", rel=lambda x: x and "icon" in x.lower())
        if not icon_link or not icon_link.get("href"):
            return 0
        href = icon_link["href"]
        parsed = urlparse(href)
        icon_domain = parsed.netloc or domain
        if domain in icon_domain:
            return -1
        else:
            return 1
    except Exception:
        return 0

# ---------------------------------------------------
# 11. port
# ---------------------------------------------------
def feat_port(url):
    parsed = urlparse(url)
    port = parsed.port
    scheme = parsed.scheme
    if port is None:
        return -1  # é»˜è®¤ç«¯å£
    if (scheme == "http" and port == 80) or (scheme == "https" and port == 443):
        return -1
    return 1

# ---------------------------------------------------
# 12. HTTPS_tokenï¼ˆåŸŸåä¸­æ»¥ç”¨ https å­—æ ·ï¼‰
# ---------------------------------------------------
def feat_HTTPS_token(url):
    domain = get_domain(url)
    scheme = get_scheme(url)
    if "https" in domain and scheme != "https":
        return 1
    return -1

# ---------------------------------------------------
# 13. Request_URLï¼ˆå¤–éƒ¨èµ„æºæ¯”ä¾‹ï¼‰
# ---------------------------------------------------
def feat_Request_URL(url, soup):
    try:
        domain = get_domain(url)
        total = 0
        external = 0
        tags = soup.find_all(["img", "audio", "video", "embed", "iframe"])
        for tag in tags:
            src = tag.get("src")
            if not src:
                continue
            total += 1
            src_domain = urlparse(src).netloc
            if src_domain and domain not in src_domain:
                external += 1
        if total == 0:
            return -1
        ratio = external / total
        if ratio < 0.22:
            return -1
        elif 0.22 <= ratio <= 0.61:
            return 0
        else:
            return 1
    except Exception:
        return 0

# ---------------------------------------------------
# 14. URL_of_Anchor
# ---------------------------------------------------
def feat_URL_of_Anchor(url, soup):
    try:
        domain = get_domain(url)
        anchors = soup.find_all("a")
        if not anchors:
            return -1
        unsafe = 0
        for a in anchors:
            href = a.get("href")
            if not href or href == "#" or href.startswith("javascript:"):
                unsafe += 1
                continue
            href_domain = urlparse(href).netloc
            if href_domain and domain not in href_domain:
                unsafe += 1
        ratio = unsafe / len(anchors)
        if ratio < 0.31:
            return -1
        elif 0.31 <= ratio <= 0.67:
            return 0
        else:
            return 1
    except Exception:
        return 0

# ---------------------------------------------------
# 15. Links_in_tagsï¼ˆmeta/script/linkï¼‰
# ---------------------------------------------------
def feat_Links_in_tags(url, soup):
    try:
        domain = get_domain(url)
        tags = []
        tags += soup.find_all("meta")
        tags += soup.find_all("script")
        tags += soup.find_all("link")

        if not tags:
            return -1

        total = 0
        external = 0
        for t in tags:
            for attr in ["src", "href", "content"]:
                v = t.get(attr)
                if not v:
                    continue
                if "http" in v:
                    total += 1
                    v_domain = urlparse(v).netloc
                    if v_domain and domain not in v_domain:
                        external += 1

        if total == 0:
            return -1
        ratio = external / total
        if ratio < 0.17:
            return -1
        elif 0.17 <= ratio <= 0.81:
            return 0
        else:
            return 1
    except Exception:
        return 0

# ---------------------------------------------------
# 16. SFHï¼ˆè¡¨å•æäº¤åœ°å€ï¼‰
# ---------------------------------------------------
def feat_SFH(url, soup):
    try:
        domain = get_domain(url)
        forms = soup.find_all("form")
        if not forms:
            return -1
        suspicious = 0
        for f in forms:
            action = f.get("action")
            if action is None or action == "" or action == "about:blank":
                return 1
            if "http" in action:
                a_domain = urlparse(action).netloc
                if domain not in a_domain:
                    suspicious += 1
        if suspicious == 0:
            return -1
        else:
            return 1
    except Exception:
        return 0

# ---------------------------------------------------
# 17. Submitting_to_email
# ---------------------------------------------------
def feat_Submitting_to_email(soup):
    try:
        forms = soup.find_all("form")
        for f in forms:
            action = f.get("action", "")
            if "mailto:" in action:
                return 1
        # JS ä¸­ mail()
        if "mailto:" in soup.get_text():
            return 1
        return -1
    except Exception:
        return 0

# ---------------------------------------------------
# 18. Abnormal_URLï¼ˆWHOIS ä¸­æ— è¯¥åŸŸåç­‰ï¼Œç®€åŒ–ï¼‰
# ---------------------------------------------------
def feat_Abnormal_URL(url):
    domain = get_domain(url).split(":")[0]
    try:
        w = whois.whois(domain)
        if w.domain_name:
            return -1
        else:
            return 1
    except Exception:
        return 1

# ---------------------------------------------------
# 19. Redirectï¼ˆHTTP è·³è½¬æ¬¡æ•°ï¼‰
# ---------------------------------------------------
def feat_Redirect(url):
    try:
        resp = requests.get(url, timeout=10, verify=False, allow_redirects=True)
        # resp.history é‡Œä¿å­˜äº†ä¸­é—´è·³è½¬çš„å“åº”
        redirects = len(resp.history)
        if redirects <= 1:
            return -1
        elif redirects == 2:
            return 0
        else:
            return 1
    except Exception:
        return 0

# ---------------------------------------------------
# 20. on_mouseoverï¼ˆJSï¼‰
# ---------------------------------------------------
def feat_on_mouseover(soup):
    try:
        scripts = soup.find_all("script")
        for s in scripts:
            txt = s.get_text().lower()
            if "onmouseover" in txt and ("window.status" in txt or "location.href" in txt):
                return 1
        return -1
    except Exception:
        return 0

# ---------------------------------------------------
# 20. RightClickï¼ˆç¦ç”¨å³é”®ï¼‰
# ---------------------------------------------------
def feat_RightClick(soup):
    try:
        scripts = soup.find_all("script")
        for s in scripts:
            txt = s.get_text().lower()
            if "event.button==2" in txt or "contextmenu" in txt:
                return -1
        body = soup.find("body")
        if body and ("contextmenu" in body.attrs or "oncontextmenu" in body.attrs):
            return 1
        return -1
    except Exception:
        return 0

# ---------------------------------------------------
# 21. popUpWidnowï¼ˆå¼¹çª—ï¼‰
# ---------------------------------------------------
def feat_popUpWidnow(soup):
    try:
        scripts = soup.find_all("script")
        for s in scripts:
            txt = s.get_text().lower()
            if "alert(" in txt or "window.open(" in txt:
                return 1
        return -1
    except Exception:
        return 0

# ---------------------------------------------------
# 22. Iframe
# ---------------------------------------------------
def feat_Iframe(soup):
    try:
        iframes = soup.find_all("iframe")
        if not iframes:
            return -1
        # ç®€å•ï¼šå­˜åœ¨ iframe å°±è®¤ä¸ºå¯ç–‘
        return 1
    except Exception:
        return 0

# ---------------------------------------------------
# 23. age_of_domainï¼ˆWHOISï¼Œç®€åŒ–ï¼‰
# ---------------------------------------------------
def feat_age_of_domain(url):
    domain = get_domain(url).split(":")[0]
    try:
        w = whois.whois(domain)
        created = w.creation_date
        if isinstance(created, list):
            created = created[0]
        if not created:
            return 0
        age_months = (datetime.now() - created).days / 30.0
        if age_months < 6:
            return 1
        else:
            return -1
    except Exception:
        return 0

# ---------------------------------------------------
# 24. DNSRecord
# ---------------------------------------------------
def feat_DNSRecord(url):
    domain = get_domain(url).split(":")[0]
    try:
        dns.resolver.resolve(domain, "A")
        return -1
    except Exception:
        return 1

# ---------------------------------------------------
# 25. web_trafficï¼ˆæ— Alexaç­‰APIæ—¶ï¼šä¸å†é»˜è®¤æœªçŸ¥=0ï¼‰
# è§„åˆ™ï¼š
# - æœ‰å¯ç”¨çš„â€œtrancoæ’åæ–‡ä»¶â€å°±æŒ‰æ’ååˆ†æ®µ
# - æ²¡æœ‰æ’åæ•°æ®ï¼šé»˜è®¤è¿”å› 1ï¼ˆä¸å› ç¼ºæ•°æ®æ‰£åˆ†ï¼‰
# ---------------------------------------------------

@lru_cache(maxsize=20000)
def _load_tranco_map():
    """
    å¯é€‰ï¼šå¦‚æœä½ æ”¾ä¸€ä¸ª Tranco top list æ–‡ä»¶åˆ° data/tranco_top1m.csv
    æ ¼å¼ï¼šrank,domain   (ä¾‹å¦‚ï¼š1,google.com)
    å°±èƒ½å¯ç”¨â€œæµé‡/çƒ­åº¦â€è¿‘ä¼¼åˆ¤æ–­ã€‚
    """
    candidates = [
        os.path.join(os.path.dirname(__file__), "data", "tranco_top1m.csv"),
        os.path.join(os.path.dirname(__file__), "tranco_top1m.csv"),
    ]
    path = next((p for p in candidates if os.path.exists(p)), None)
    if not path:
        return None

    m = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or "," not in line:
                continue
            rank_s, dom = line.split(",", 1)
            dom = dom.strip().lower()
            try:
                m[dom] = int(rank_s)
            except Exception:
                continue
    return m

def feat_web_traffic(url):
    domain = get_domain(url).split(":")[0].lower()
    tranco = _load_tranco_map()
    if not tranco:
        return -1  # å…³é”®ï¼šæ— æ•°æ®æ—¶ä¸æ‰£åˆ†ï¼Œé¿å…æ¨¡å‹å› â€œå…¨0â€åå‘é’“é±¼

    rank = tranco.get(domain)
    if rank is None:
        return 0  # ä¸åœ¨æ¦œå•ï¼šä¿¡æ¯ä¸è¶³ï¼Œç»™0è€Œä¸æ˜¯-1
    if rank <= 100000:
        return -1
    elif rank <= 1000000:
        return 0
    else:
        return 1


# ---------------------------------------------------
# 26. Page_Rankï¼ˆæ— æœç´¢å¼•æ“APIï¼šç”¨â€œtrancoæ’åâ€è¿‘ä¼¼ï¼‰
# è§„åˆ™ï¼šåŒä¸Šï¼Œè¶Šçƒ­é—¨è¶Šåƒæ­£å¸¸
# ---------------------------------------------------
def feat_Page_Rank(url):
    domain = get_domain(url).split(":")[0].lower()
    tranco = _load_tranco_map()
    if not tranco:
        return -1  # æ— æ•°æ®æ—¶ä¸æ‰£åˆ†

    rank = tranco.get(domain)
    if rank is None:
        return 0
    if rank <= 200000:
        return -1
    elif rank <= 1000000:
        return 0
    else:
        return 1


# ---------------------------------------------------
# 27. Google_Indexï¼ˆä¸çˆ¬è°·æ­Œï¼šç”¨ç«™ç‚¹è‡ªèº«â€œnoindexâ€ä¿¡å·åˆ¤æ–­ï¼‰
# è§„åˆ™ï¼š
# - é¡µé¢æ˜¾å¼å£°æ˜ noindex / X-Robots-Tag:noindex => -1
# - å¦åˆ™ => 1ï¼ˆä¸å†é»˜è®¤0ï¼‰
# ---------------------------------------------------
def feat_Google_Index(url):
    try:
        resp = requests.get(url, timeout=10, verify=False, allow_redirects=True, headers={
            "User-Agent": "Mozilla/5.0"
        })
        # HTTPå¤´ noindex
        xrt = resp.headers.get("X-Robots-Tag", "")
        if "noindex" in xrt.lower():
            return 1

        soup = BeautifulSoup(resp.text or "", "html.parser")
        meta = soup.find("meta", attrs={"name": lambda x: x and x.lower() == "robots"})
        if meta and meta.get("content") and "noindex" in meta["content"].lower():
            return 1

        return -1
    except Exception:
        return 0  # çœŸè®¿é—®å¤±è´¥æ—¶æ‰ç»™0


# ---------------------------------------------------
# 28. Links_pointing_to_pageï¼ˆä¸åšå¤–éƒ¨åé“¾APIï¼šæ”¹ä¸ºâ€œç«™å†…å¼•ç”¨å¼ºåº¦â€è¿‘ä¼¼ï¼‰
# è§„åˆ™ï¼š
# - èƒ½æŠ“åˆ°é¡µé¢ï¼šç»Ÿè®¡â€œå†…éƒ¨é“¾æ¥æ•°é‡â€
#   å†…éƒ¨é“¾æ¥å¾ˆå¤š => æ›´åƒæ­£å¸¸ï¼ˆ1ï¼‰
#   å°‘é‡ => 0
#   å‡ ä¹æ²¡æœ‰ä¸”é¡µé¢å¾ˆåƒè½åœ°é¡µ => -1ï¼ˆè°¨æ…ï¼‰
# - æŠ“ä¸åˆ°é¡µé¢ => 0
# ---------------------------------------------------
def feat_Links_pointing_to_page(url):
    try:
        domain = get_domain(url).split(":")[0].lower()
        resp = requests.get(url, timeout=10, verify=False, allow_redirects=True, headers={
            "User-Agent": "Mozilla/5.0"
        })
        soup = BeautifulSoup(resp.text or "", "html.parser")
        anchors = soup.find_all("a")
        if not anchors:
            return 0

        internal = 0
        for a in anchors:
            href = a.get("href") or ""
            if not href:
                continue
            d = urlparse(href).netloc.lower()
            # ç›¸å¯¹é“¾æ¥ or åŒåŸŸ
            if (not d) or (domain in d):
                internal += 1

        if internal >= 15:
            return -1
        elif internal >= 5:
            return 0
        else:
            # å†…é“¾æå°‘ï¼šè°¨æ…ç»™ 1ï¼ˆä½†ä¸â€œä¸€åˆ€åˆ‡â€ï¼‰
            return 1
    except Exception:
        return 0


# ---------------------------------------------------
# 29. Statistical_reportï¼ˆä¸é»˜è®¤æœªçŸ¥=0ï¼Œæ”¯æŒæœ¬åœ°é»‘åå•ï¼‰
# è§„åˆ™ï¼š
# - è‹¥å­˜åœ¨ blacklist.txtï¼ˆä¸€è¡Œä¸€ä¸ªdomainï¼‰ï¼Œå‘½ä¸­ => -1
# - æœªå‘½ä¸­æˆ–æ— æ–‡ä»¶ => 1ï¼ˆä¸æ‰£åˆ†ï¼‰
# ---------------------------------------------------
@lru_cache(maxsize=1)
def _load_blacklist():
    candidates = [
        os.path.join(os.path.dirname(__file__), "blacklist.txt"),
        os.path.join(os.path.dirname(__file__), "data", "blacklist.txt"),
    ]
    path = next((p for p in candidates if os.path.exists(p)), None)
    if not path:
        return set()

    s = set()
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            d = line.strip().lower()
            if d and not d.startswith("#"):
                s.add(d)
    return s

def feat_Statistical_report(url):
    domain = get_domain(url).split(":")[0].lower()
    bl = _load_blacklist()
    if not bl:
        return -1
    return 1 if domain in bl else -1


# ---------------------------------------------------
# ä¸»å‡½æ•°ï¼šç»™å®š URLï¼Œè¿”å› 29 ä¸ªç‰¹å¾çš„ dict
# ---------------------------------------------------
def extract_features(url):
    features = {}

    # å°è¯•è·å–ç½‘é¡µå†…å®¹
    soup = None
    try:
        resp = requests.get(url, timeout=10, verify=False)
        html = resp.text
        soup = BeautifulSoup(html, "html.parser")
    except Exception:
        # æ— æ³•è®¿é—®é¡µé¢æ—¶ï¼Œå’Œå†…å®¹ç›¸å…³çš„ç‰¹å¾ç”¨ None / 0
        soup = BeautifulSoup("", "html.parser")

    features["having_IP_Address"] = feat_having_IP_Address(url)
    features["URL_Length"] = feat_URL_Length(url)
    features["Shortining_Service"] = feat_Shortining_Service(url)
    features["having_At_Symbol"] = feat_having_At_Symbol(url)
    features["double_slash_redirecting"] = feat_double_slash_redirecting(url)
    features["Prefix_Suffix"] = feat_Prefix_Suffix(url)
    features["having_Sub_Domain"] = feat_having_Sub_Domain(url)
    features["SSLfinal_State"] = feat_SSLfinal_State(url)
    features["Domain_registeration_length"] = feat_Domain_registeration_length(url)
    features["Favicon"] = feat_Favicon(url, soup)
    features["port"] = feat_port(url)
    features["HTTPS_token"] = feat_HTTPS_token(url)
    features["Request_URL"] = feat_Request_URL(url, soup)
    features["URL_of_Anchor"] = feat_URL_of_Anchor(url, soup)
    features["Links_in_tags"] = feat_Links_in_tags(url, soup)
    features["SFH"] = feat_SFH(url, soup)
    features["Submitting_to_email"] = feat_Submitting_to_email(soup)
    features["Abnormal_URL"] = feat_Abnormal_URL(url)
    features["Redirect"] = feat_Redirect(url)
    features["on_mouseover"] = feat_on_mouseover(soup)
    features["RightClick"] = feat_RightClick(soup)
    features["popUpWidnow"] = feat_popUpWidnow(soup)
    features["Iframe"] = feat_Iframe(soup)
    features["age_of_domain"] = feat_age_of_domain(url)
    features["DNSRecord"] = feat_DNSRecord(url)
    features["web_traffic"] = feat_web_traffic(url)
    features["Page_Rank"] = feat_Page_Rank(url)
    features["Google_Index"] = feat_Google_Index(url)
    features["Links_pointing_to_page"] = feat_Links_pointing_to_page(url)
    features["Statistical_report"] = feat_Statistical_report(url)

    return features

def extract_features_from_file(url_file_path):
    """
    ä»æ–‡ä»¶ä¸­é€è¡Œè¯»å– URLï¼Œå¹¶æå–ç‰¹å¾
    """
    results = []

    with open(url_file_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, start=1):
            url = line.strip()
            if not url:
                continue  # è·³è¿‡ç©ºè¡Œ

            print(f"æ­£åœ¨å¤„ç†ç¬¬ {line_num} è¡Œ: {url}")
            try:
                feats = extract_features(url)
                feats["Result"] = None  # ç•™ç©ºï¼šå¾…è¯„ä¼°æ ‡ç­¾
                results.append(feats)
            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")

    return results


if __name__ == "__main__":
    import os
    import pandas as pd

    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    url_file = os.path.join(BASE_DIR, "url.txt")

    if not os.path.exists(url_file):
        print(f"âŒ URL æ–‡ä»¶ä¸å­˜åœ¨: {url_file}")
        exit(1)

    all_features = extract_features_from_file(url_file)

    if not all_features:
        print("âŒ æœªæå–åˆ°ä»»ä½•ç‰¹å¾")
        exit(1)

    # è½¬ä¸º DataFrameï¼ˆéå¸¸é‡è¦ï¼Œåç»­å¯ç›´æ¥å–‚ç»™æ¨¡å‹ï¼‰
    df = pd.DataFrame(all_features)

    # æŒ‰ FEATURE_NAMES é¡ºåºæ’åˆ—ï¼ˆä¸å« labelï¼‰
    df = df[FEATURE_NAMES + ["Result"]]

    # ä¿å­˜ç»“æœ
    output_csv = os.path.join(BASE_DIR, "independent_test.csv")
    df.to_csv(output_csv, index=False, encoding="utf-8-sig")

    print(f"\nâœ… ç‰¹å¾æå–å®Œæˆï¼Œå…± {len(df)} ä¸ª URL")
    print(f"ğŸ“„ å·²ä¿å­˜åˆ°: {output_csv}")
